# 数据预处理详解

> 原文：[`developer.ibm.com/zh/articles/data-preprocessing-in-detail/`](https://developer.ibm.com/zh/articles/data-preprocessing-in-detail/)

## 简介

由于当今数据的数量庞大且来自于各种不同类型的来源，因此出现数据异常的可能性不断增加。鉴于高质量数据可生成更好的模型和预测，数据预处理的重要性与日俱增，并且已经成为数据科学/机器学习/AI 管道中的基本步骤。在本文中，我们将探讨数据处理需求，并讨论用于完成此流程中每个步骤的不同方法。

在数据收集过程中，存在三个影响数据质量的主要因素：

1.  **准确率**：与期望值之间存在偏差的错误值。数据不准确的原因多种多样，包括：

    *   数据输入和传输期间发生的人为错误/计算机错误
    *   用户有意提交错误值（称为“伪装缺失数据”）
    *   输入字段格式错误
    *   训练示例重复
2.  **完整性**：缺少属性值/特征值/关联值。数据集可能因以下原因而不完整：

    *   数据不可用
    *   删除不一致数据
    *   删除最初被认为无关的数据
3.  **一致性**：数据聚合不一致。

影响数据质量的一些其他特征还包括及时性（在某些时间段之后但在提交所有相关信息之前数据不完整）、可信度（用户信任的数据量）以及可解释性（所有利益相关方是否都能轻松理解数据）。

为确保获得高质量的数据，对数据进行预处理就显得至关重要。为了简化此流程，数据预处理分为四个阶段：数据清理、数据集成、数据缩减和数据转换。

## 数据清理

“数据清理”是指用于“清理”数据的方法，具体包括移除异常值、替换缺失值、将干扰数据进行平滑处理以及纠正不一致数据。在执行上述每一项任务的过程中会使用到多种不同方法，将根据用户偏好或问题集来确定具体使用的方法。下面从问题解决方法的角度介绍了每一项任务。

### 缺失值

为了解决缺失数据的问题，可采用多种方法。让我们来逐一了解这些方法。

1.  **移除训练示例**：如果缺少输出标签（如果这属于分类问题），那么可以忽略训练示例。通常不鼓励采用此方法，因为它会导致数据丢失，因为移除的属性值也可以为数据集带来价值。
2.  **手动填充缺失值**：此方法较为耗时。对于大型数据集，建议不要采用此方法。
3.  **使用标准值来替换缺失值**：缺失值可替换为全局常量（例如“N/A”或“Unknown”）。这是一种简单方法，但并非万无一失。
4.  **使用属性的集中趋势（平均值、中间值、众数）来替换缺失值**：根据数据分布，可使用平均值（适用于正态分布）或中间值（适用于非正态分布）来填充缺失值。
5.  **使用同类属性的集中趋势（平均值、中间值、众数）来替换缺失值**：此方法与方法 4 相同，但集中趋势的度量值因每个类而异。
6.  **使用最可能的值来填充缺失值**：可使用回归和决策树等算法来预测并替换缺失值。

### 干扰数据

干扰定义为所度量的变量中的随机方差。对于数字值，可使用箱线图和散点图来识别异常值。为处理这些异常值，可按下述方式应用数据平滑技术。

1.  **分箱**：可使用分箱方法，利用排序值周围的值来对该排序值进行平滑处理。这样，排序值便可以分为多个“箱”。有多种方法可用于分箱。其中有一种方法采用箱平均值进行平滑处理，即把每个箱替换为该箱中的值的平均值，还有一种方法采用箱中间值进行平滑处理，即把每个箱替换为该箱中的值的中间值。
2.  **回归**：可使用线性回归和多重线性回归来对数据进行平滑处理，其中所有值都符合某一函数。
3.  **异常值分析**：可使用分群等方法来检测和处理异常值。

## 数据集成

由于数据是从多种来源收集而来，因此“数据集成”已经成为数据预处理流程中的一个重要组成部分。这可能导致出现冗余数据和不一致数据，从而导致数据模型的准确性和速度都有所下降。为解决此类问题并保持数据完整性，随后需要使用诸如元组重复检测和数据冲突检测等方法。下面解释了最常用的数据集成方法。

1.  **数据整合**：以物理方式将数据一起导入到同一个数据存储。这通常涉及数据仓储技术。
2.  **数据传播**：使用称为“数据传播”的应用程序将数据从一个位置复制到另一个位置。此过程可同步或异步执行，并且属于事件驱动型操作。
3.  **数据虚拟化**：使用界面提供来自多种不同来源的数据的实时的统一视图。可从单一访问点来查看数据。

## 数据缩减

“数据缩减”是为了以精简方式表示数据集，这样得到的数据量较小，同时又保留了原始数据的完整性。这样可得到高效且相似的结果。减少数据量的方法包括：

1.  **缺失值比率**：移除所含缺失值数量超过阈值的属性。
2.  **低方差过滤器**：也会移除其方差（分布）小于阈值的规范化属性，因为数据变化较少就意味着信息较少。
3.  **高相关过滤器**：也会移除其相关系数超过阈值的规范化属性，因为相似的趋势意味着携带相似的信息。通常使用 Pearson 卡方值等统计方法来计算相关系数。
4.  **主成份分析**：主成份分析 (PCA) 是一种统计方法，它通过将高度相关的属性集中在一起来减少属性数量。通过每次迭代，初始特征缩减为主成份，并且其方差大于原始集合，前提是这些主成份与原先成份不相关。但此方法仅适用于具有数字值的特征。

## 数据转换

数据预处理流程的最后一个步骤是将数据转换为适合数据建模的格式。支持数据转换的策略包括：

1.  **平滑处理**
2.  **属性/特征构造**：通过给定属性集来构造新属性。
3.  **聚合**：针对给定属性集应用“摘要”和“聚合”操作来构造新属性。
4.  **规范化**：每个属性中的数据都会缩放到较小的范围（例如，0 到 1 或 -1 到 1）内。
5.  **离散化**：数字属性的原始值将替换为离散或概念区间，以便进一步组织为更高级的区间。
6.  **名义数据的概念层次结构生成**：名义数据值将归纳为更高阶的概念。

尽管可使用多种方法来进行数据预处理，但由于每天都会生成大量的不连贯数据，因此数据预处理当前仍属于一个热门研究领域。为促进这一领域的研究，IBM Cloud 为数据科学家提供了一个名为 IBM Watson Studio 的平台，其中包含各种服务以支持数据科学家在传统编程方法的基础上，进一步使用拖放式服务来对数据进行预处理。要深入了解 Watson Studio 以及它如何为数据科学生命周期提供帮助，请[访问](https://www.ibm.com/cloud/watson-studio/details)。

本文翻译自 ：[Data preprocessing in detail](https://developer.ibm.com/articles/data-preprocessing-in-detail/)（2019-06-14）