# 使用 Hive 分析大型数据集

> 原文：[`developer.ibm.com/zh/tutorials/bd-hiveanalyze/`](https://developer.ibm.com/zh/tutorials/bd-hiveanalyze/)

## 简介

大数据行业已掌握了收集和记录数兆字节数据的能力，但根据这些实时数据进行基本预测和制定决策仍是一项挑战，这也是 Apache Hive 如此之重要的原因。它将结构应用于数据，按照类似 SQL 的查询结构来查询此数据，以便在大型数据集上执行 Map 和 Reduce 任务。

InfoSphere BigInsights 是来自 IBM 的一个相似的专有系统。它基于开源 Apache Hadoop 项目，但包含用于企业部署的一些新增功能。它使得使用 Hadoop 以及构建由大型数据集驱动的应用程序变得更容易。和 Hive 一样，它为 Hadoop 提供了一个 SQL 界面，这样，用户无需学习新编程语言就可以访问 BigInsights 中的数据。它还针对无缝和故障转移技术为 BigInsights NameNode（又称为 MasterNode）提供了高可用性，从而减少了系统停机时间。

我们将介绍如何使用 Hive 分析调用数据记录 (CDR)，以此作为实施大数据分析的一个示例。CDR 是电信领域内使用的一个术语，指可能用于向订户收取费用的任何事件。诸如呼叫启动、终止、持续和通过智能电话传输 Internet 数据之类的事件，都属于在数据存储中记录的服务以及用于计费用户的服务的示例。

## 开始之前

建议您掌握以下技术和概念的基本知识。请参阅 参考资料 部分，获取有关以下内容的教程：

*   如何编写基本 SQL DDL 和 DML 脚本（如 `select, create, insert` ）。
*   如何编写和编译基本 Java™ 程序。
*   如何将 Java 字节码包装成 Java Archive (JAR) 文件。

## 使用 CDR 作为一个示例

##### InfoSphere BigInsights Quick Start Edition

InfoSphere <reg>BigInsights <trade>Quick Start Edition 是 InfoSphere BigInsights 的免费赠送的可下载版本。使用 Quick Start Edition，您可以试用 IBM 为增加开源 Hadoop 的价值而构建的功能，如 Big SQL、文本分析和 BigSheets。我们提供的引导学习会尽可能地让您的体验更加顺利，这些体验包括分步的、自助的教程和视频，可帮助您开始让 Hadoop 为您效力。没有任何时间和数据限制，您可以在您有时间的时候用大量数据进行试验。[学习教程 (PDF)](http://www.ibm.com/e-business/linkweb/publications/servlet/pbi.wss?CTY=US&FNC=SRX&PBL=GC19-4104-00)，并 [立即下载 BigInsights Quick Start Edition](http://www.ibm.com/developerworks/cn/downloads/im/biginsightsquick/)。</trade></reg>

*呼叫详细记录* (CDR)，也被称为 *呼叫数据记录* ，是电话交换台或其他电信设备产生的数据记录，记录了通过设施或设备的电话的详细信息。CDR 由描述电信事务的数据字段组成。这些数据字段可以包含以下信息：

*   订户电话号码
*   接听方电话号码
*   起始时间戳
*   呼叫持续时间
*   付费电话号码
*   电话交换台设备 ID
*   记录 ID
*   呼叫处置或结果（不论呼叫状态是繁忙还是失败）
*   呼叫进入交换台的线路
*   呼叫离开交换台的线路
*   呼叫类型（语音、消息等）

基于本文以及 CDR 的目的，我们还要使用另外一个称为网络日志的数据集。网络日志是在用户拨打电话、访问 Web、访问电子邮件或简单转移到另一个蜂窝站时的网络中心活动的记录。网络日志的一些相关数据字段如下所示：

*   事件的时间戳
*   IMSI（与蜂窝站相关的惟一 ID）
*   IMEI（标识移动电话的惟一 ID）
*   呼叫类型（语音呼叫、消息等的代码）
*   蜂窝站类型（记录此信息的蜂窝站类型的代码）
*   蜂窝站 ID（记录此信息的蜂窝站的 ID）
*   订户电话号码
*   纬度（蜂窝站的地理坐标）
*   经度（蜂窝站的地理坐标）

电信提供商对评估各种趋势感兴趣，以便规划未来的升级和真实数据驱动的部署。例如，典型的提供商会希望了解哪台设备（如蜂窝站）发出了大部分呼叫。另一个有价值的数据点是确定哪些基站在当天不同的时段内（尤其是昼夜通信模式下）是最繁忙的中心，以及哪类呼叫经常发送到不同地点。我们将关联这两个大数据源，对于一个中等规模的蜂窝网络，这两个大数据源每天通常产生大约数百千兆字节的数据。

相关的值得一提的事情是，这些呼叫日志包含任何蜂窝网络上执行的每个蜂窝事务 (cellular transaction) 的数据。在最近的新闻中，Verizon 前所未有地访问了 National Security Agency (NSA)，以便直接从 Verizon 的服务器获取呼叫日志。这被转化成了数百万个指定了呼叫方和接听方的号码、通话持续时间以及端点站的地点的呼叫日志条目。

## 使用 Hive 进行大数据分析

让我们先从关联上面定义的两个数据集开始。为此，Hive 提供了类似 SQL 的连接语义。内连接是应用程序中使用的最常见的 `join` 操作，可将它视为默认连接类型。内连接基于连接谓词将两个表（假设为 A (CDR) 和 B（网络日志））的列值合并在一起。内部 `join` 查询将 A 表与 B 表的每一行进行比较，找出满足连接谓词的所有行对。如果满足连接谓词，则会将该记录的 A 和 B 的列值合并，以建立新的合成记录。可以这样思考内连接：它获取这两个表的 Cartesian 产品，然后返回满足连接谓词的记录。下面显示了连接查询的简单表示。

### 内连接谓词

`join` 谓词是一个条件，由 ‘on’ 定义，并在清单 1 的查询中编写。

##### 内连接谓词

```
hive> insert overwrite table correlation            \
      partition(dt=20130101, hour=12)                  \
      select cdr.timestamp,      cdr.subscriberPhone,  \
      cdr.recipientPhone,        cdr.duration,         \
      cdr.billingPhone,          cdr.exchangeID,       \
      cdr.recordID,              cdr.callResult,       \
      cdr.entryRoute,            cdr.exitRoute,        \
      cdr.callType,                                    \
      net.dtstamp,               net.minute            \
     net.second,                 net.IMSI,             \
     net.IMEI,                   net.callType,         \
     net.cellType,               net.cellID,           \
     net.subscriberPhone,        net.cellLat,          \
     net.cellLon                                       \
      from cdr join net                                \
on (cdr.subscriberPhone = net.subscriberPhone)         \
      where combine(net.dtstamp,                       \
            net.hour,                                  \
            net.minute,                                \
            net.second) <= cdr.timestamp            \
      and cdr.timestamp like '2013010112%              \
      and net.dtstamp like '20130101'                  \
      and net.hour = 12; 
```

让我们详细介绍一下查询。查询语句的 `insert overwrite` 部分只告知 Hive 解释器将选定字段插入 `table` 关键字后的表中，在这里，关键字为 `correlation` 。 `join` 关键字和 `where` 子句严格遵守了标准 SQL 语法。

关联表可使用 `create table` 标准 SQL 命令创建。在上面的查询中，有一个 `partition` 关键字，它定义了数据应插入到关联表的哪个分区。一个表可以有一个或多个分区列，对于分区列中的每个不同值组合，可以创建一个单独的数据目录。清单 2 是如何创建包含分区的表的一个示例。我们将使用关联表用作一个示例。

##### 表的相互关系

```
hive> create table correlation(
timestamp string,     subscriberPhone string,         \
      recipientPhone string,     duration int,        \
      billingPhone string,     exchangeID string,     \
      recordID string,         callResult string,     \
      entryRoute string,     exitRoute string,        \
    callType string,         dtstamp string,          \
minute int,            second int,                    \
IMSI string,        IMEI string,                      \
callType string,        cellType string,              \
cellID string,        subscriberPhone string,         \
cellLat string,        cellLon string)                \
partitioned by (dt string, hour int)                  \
location '/mnt/user/hive/warehouse/correlation' 
```

我们还使用了自定义 Hive 的用户定义函数 (UDF) （参见 参考资料 ，了解有关的更多信息）。UDF 是由用户编写的自定义函数，可加载到 Hive 命令行接口 (CLI) 中，并且可以重复使用。它和 Java 类一样是一个通用术语。我们的 UDF 被定义为 `combine()` ，它只将日期、小时、分钟和秒组合成为所需的格式，如：yyyyMMddHHMMSS。DateCombiner UDF Java 类的代码如下所示。请注意，我们扩展了 UDF Java 类。您还可以使用 GenericUDF Java 类，它是一个更加新的 UDF 语义实现。由于使用了惰性计算（lazy evaluation）和短路循环（short-circuiting），它能够提供更好的性能。它还支持非原始参数和可变数量的参数。

UDF Java 类比 GenericUDF Java 类更容易使用，而且提供了可接受的性能。它使用了 Reflection API，因此比 GenericUDF Java 实现稍慢。它不接受或返回像 array、struct 或 maps 这样的非原始参数，不支持数目不定的参数。如果需要处理复杂的数据类型，您可以使用 GenericUDF Java 类，不过，对于大多数实例，都可以使用 UDF Java 类。

编写 UDF 时要做的第一件事就是定义 Java 包。接下来，将 UDF Java 类导入到源代码，以告知 Java 编译器哪个 Java 父类负责继承类使用的方法。导入声明只是源代码的一个编译时间元素，在运行期间不存在。这是因为 JVM 字节码总是使用完全限定类名。

接下来，将会申明扩展父 UDF Java 类的类。定义一个 `evaluate()` 方法，它包含程序逻辑。简单来说，UDF 必须满足以下两个属性：

1.  UDF 必须是 UDF Java 类或 GenericUDF Java 类的子类。
2.  UDF 必须至少实现一个 `evaluate()` 方法（参见清单 3）。

##### 定义一个 `evaluate()` 方法

```
package HiveUDF;
import org.apache.hadoop.hive.ql.exec.UDF;
public final class DateCombiner extends UDF (
public String evaluate(final String dt, final String hh, final String mm,
        final String ss) {
      it(dt==null) { return null; }
      String ho="", mi="", se="",
      if(hh.length() != 2) { ho = "0".concat(hh); } else { ho = hh; }
     if(mm.length() != 2) { mi = "0".concat(mm); } else { mi = mm; }
     if(ss.length() != 2) { se = "0".concat(ss); } else { se = ss; }
     return new String(dt + ho + mi + se);
  } 
```

在使用 UDF 并执行上述 `join` 查询之前，需要先执行几个重要步骤。首先，必须编译 UDF，将其包装成一个 JAR 文件。要进行编译，则要求添加 Hive 可执行 JAR 文件或在 Java 类路径中提供它们（参见清单 4）。

##### 添加 Hive 可执行 JAR 文件

```
$javac -cp /apps/hive/lib/hive-exec-0.7.1-cdh3u3.jar DateCombiner.java
$ jar cf HiveUDF.jar HiveUDF/ 
```

接下来，必须将 UDF JAR 文件加载到 Hive CLI 中。然后应该创建一个称为 `combine` 的临时函数，该函数与 DateCombiner UDF Java 类相对应（参见清单 5）。

##### 清单 5\. `combine` 函数

```
hive> add jar /home/verrami1/work/JARS/HiveUDF.jar;
hive> create temporary function combine as 'HiveUDF.DateCombiner'; 
```

现在，我们已经介绍了如何使用 UDF Java 类在大数据集上应用内连接，在我们的案例中，使用的是由两个列组成的 CDR 数据库。接下来，我们将列举一些大数据分析中发现的通用的 Hive 最佳实践。

## 通过调优 Hive 任务来提升速度和效率

在这一小节中，我们将介绍在 Hive 上运行和执行查询的一些最佳实践。虽然这些实践并不是特定于我们的电信用例，但它们同样适用于电信公司处理的海量数据。

### 数据分区

我们需要确保满足适当条件，以便在定义一个表时创建分区，这将提高处理 Hive 任务的速度。谨慎创建分区还可以减少很多纷争。对表进行分区的最佳方法是使用一个或多个指标，具体情况取决于表的预期大小。在我们的分析 CDR 和网络日志的示例中，我们预期将十亿字节的 CDR 数据记录到系统中。现在要将这一数字乘以 24 小时，假设对一天中收集的数据（约 24 GB）运行一个选择查询。此呼叫数据不会停止注入到数据库中，如果没有认真地进行数据分区，您最终可能需要一个大型集群来执行最微小的数据库查询。清单 6 演示了一个对数据进行分区的巧妙方法。

##### 清单 6\. 对数据进行分区的巧妙方法

```
hive> create table correlation(                         \
     timestamp string,           subscriberPhone string,   \
     recipientPhone string,      duration int,             \
     billingPhone string,        exchangeID string,        \
     recordID string,            callResult string,        \
     entryRoute string,          exitRoute string,         \
     callType string,                                      \
     dtstamp string,              minute int,              \
     second int,                  IMSI string,             \
     IMEI string,                 callType int,            \
     cellType int,                cellID string,           \
     subscriberPhone string, cellLat string,               \
     cellLon string)                                       \
     partitioned by(dt string, hour int)                   \
     location '/mnt/data/correlation'; 
```

每个系统都有其物理资源限制，如核心和内存。如果没有认真地对数据库进行分区，那么最终会出现因为内存溢出或用尽而导致数据分析批次作业的运行突然崩溃的情况。您会发现，尽管出现了过多的错误，但没有任何迹象表明内存溢出是原因所在。

### 详细地描述一个表

Hive 有一个非常好的特性，它允许您查看表的详细信息，如列、数据类型、表的存储位置、大小，等等。要查看此类信息，可以对表名称使用 `describe formatted` ，如清单 7 所示。由于输出量非常大，我们只展示了输出字段的一个样例。

##### `带格式的描述`

```
hive> describe formatted correlation;
# col_name        data_type       comment
timestamp         string          None
subscriberPhone   string          None
recipientPhone    string          None
duration          string          None
..
# Partition Information
# col_name        data_type       comment
dt                string           None
hour              int              None
..
CreateTime: Mon, Jun 17 2013 PST
Location: /mnt/data/correlation 
```

### 连接（右侧的较大的表）

如前所述，连接首先计算两个表的 Cartesian 产品，然后返回那些满足连接谓词的行（和组合列）。整个过程使得连接成为了一个计算密集的、内存繁忙的过程。为了有效地使用连接，必须将较大的表（根据行数）放在 `join` 运算的右侧。例如，如果表 A 有 100 万行，表 B 有 20 万行，查询的连接部分应为 `hive>... B join A ..;` 。

`join` 是关系代数中的一个复数运算符，数学家们已多次对其进行定义。但是，如果情况相反（例如， `A join B` ），Hadoop 映射和缩减任务将超出表 A 的 100 万行，以便从表 B 中查找匹配行。而 `B join A` 将遍历表 B 的 20 万行，尝试查找表 A 的匹配行。事实证明，由于必须进行较少迭代，此方法更有效一些。

### `where` 子句（含范围）

另一项重要任务是使用 `where` 子句将查询与限制绑定在一起。请注意，我们在 `join` 查询示例中使用了四个绑定条件。 `where` 子句限制查询不得使用蛮力攻击（brute-force）技术来返回答案。可以将此视为对必须从中检索数据的表的动态分区。

### 使用标准函数（如 `collect_set()` ）

除了提供许多有用的标准聚合函数（如 `count()` 、 `sum()` 、 `min()` 、 `max()` 等）之外，Hive 还提供了名为 `collect_set()` 的集合函数。它返回一组消除了重复元素的对象。例如，要查看在 2013 年 1 月 1 日中午在蜂窝站注册的不同用户，可使用清单 8 中的查询。

##### 清单 8\. `collect_set()` 函数

```
hive> select cellID, collect_set(subscriberPhone) from correlation
where dt=?20130101? and hour=12 group by cellID; 
```

### 类型转换

Hive 提供了 `cast()` 函数将字符串转换为整数、双精度浮点数或执行反向转换。类型转换是确保比较完全符合预期的最佳途径。清单 9 演示了通常如何使用 `cast()` 。表达式和类型可以是整数、长整型数字、浮点数、双精度浮点数或字符串。在显示的示例中，字符串被转换成了整数，双精度浮点数被转换成了字符串。

##### 清单 9\. `cast()` 函数

```
cast(expression as <type>)
hive> cast('1' as int)
hive> cast(75.89 as string) 
```

## 结束语

在本文中，我们介绍了如何使用 Hive 来分析大型数据集（使用 Hadoop 作为后端）。您了解了大型生产数据集的模式。通过一个示例，我们解释了如何连接两个大型数据集，形成一个关联数据集。为了运作海量关联数据，我们学习了如何创建带有分区的关联表。随后我们介绍了有效且高效地使用 Hive 的最佳实践。

本文翻译自：[Analyzing large datasets with Hive](https://developer.ibm.com/tutorials/bd-hiveanalyze/)（2013-07-24）